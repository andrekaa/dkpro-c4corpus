=== DKPro C4CorpusTools Developer's Guide


* Java 1.6 is required for compiling and running the project
* For running the Hadoop pipeline, Hadoop 2.6 is recommended
    * Running the pipeline on CommonCrawl located at S3 requires and active Amazon Web Services (AWS) account

==== Project structure

* ``dkpro-c4corpus-boilerplate`` contains a Java implementation of a state-of-the-art boilerplate removal (JusText, Pomikalek, 2011)
* ``dkpro-c4corpus-deduplication`` implements near-duplicate content detection based on SimHash
* ``dkpro-c4corpus-doc`` is this documentation
* ``dkpro-c4corpus-hadoop`` contains several Hadoop Map/Reduce jobs for running the pipeline on the CommonCrawl corpus
* ``dkpro-c4corpus-language`` provides language and encoding detection functionality
* ``dkpro-c4corpus-license`` implements Creative Commons license detection in html pages
* ``dkpro-c4corpus-warc-io`` contains I/O tools for reading WARC file format

Except ``dkpro-c4corpus-hadoop`` the modules are independent of each other and can run locally without Hadoop.



==== Run the whole pipeline on CommonCrawl corpus

```
$ mvn package
```

It will produce a fat jar ``dkpro-c4corpus-hadoop-1.0.0.jar`` in ``dkpro-c4corpus-hadoop/target/``.

Upload the ``dkpro-c4corpus-hadoop-1.0.0.jar`` file into your S3 bucket.

Phase 1: License detection, language detection, and boilerplate removal::

* If you are confident with AWS Elastic Map Reduce command line, you can use the following script
(slightly modified version of what we used)

```
aws emr create-cluster \
    --applications Name=Hadoop \
    --ec2-attributes \
        '{"KeyName":"your-keypair-name", \                                         <---- change this
        "InstanceProfile":"EMR_EC2_DefaultRole", \
        "SubnetId":"subnet-xxxxx", \                                               <---- change this
        "EmrManagedSlaveSecurityGroup":"sg-xxxxxx", \                              <---- change this
        "EmrManagedMasterSecurityGroup":"sg-xxxxxx"}' \                            <---- change this
    --service-role EMR_DefaultRole \
    --enable-debugging \
    --release-label emr-4.2.0 \
    --log-uri 's3n://your-logs/elasticmapreduce/' \                                <---- change this
    --steps '[\
        {"Args":["de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob", \
        "-D","mapreduce.task.timeout=7200000", \
        "-D", "mapreduce.map.failures.maxpercent=5", \
        "-D", "mapreduce.map.maxattempts=2", \
        "-D", "c4corpus.keepminimalhtml=true", \                      <---- change this (optionally)
        "s3://commoncrawl/crawl-data/CC-MAIN-2015-27/segments/*/warc/*.warc.gz",\
        "s3://ukp-research-data/c4corpus/cc-phase1out-2015-11"], \                 <---- change this
        "Type":"CUSTOM_JAR", \
        "ActionOnFailure":"CANCEL_AND_WAIT", \
        "Jar":"s3://path-to-your/dkpro-c4corpus-hadoop-1.0.0.jar", \               <---- change this
        "Properties":"", \
        "Name":"Custom JAR"}]' \
    --name 'Full cluster phase 1' \
    --instance-groups '[\
        {"InstanceCount":32, \                                        <---- change this (optionally)
            "bid-price":"your-bid-value", \                                        <---- change this
            "InstanceGroupType":"TASK",\
            "InstanceType":"c3.8xlarge", \
            "Name":"c3.8xlarge = 32 CPU"}, \
        {"InstanceCount":2, \
            "InstanceGroupType":"CORE",\
            "InstanceType":"m3.xlarge", \
            "Name":"Core instance group - 2"}, \
        {"InstanceCount":1, \
            "InstanceGroupType":"MASTER", \
            "InstanceType":"m3.xlarge", \
            "Name":"Master instance group - 1"}]' \
    --auto-terminate \
    --region us-east-1
```

* Enter your correct ``EmrManagedSlaveSecurityGroup`` and ``SubnetId``
* Path to logs and packed ``dkpro-c4corpus-hadoop-1.0.0.jar``, output path
* ``bid-price`` if you want to use Spot instances (highly recommended, but might get unstable)
    ** If Spot instances died (were over-bidden), the entire job went unstable and failed,
    I recommend to put your bid higher then usual to make sure you won't lose instances
* Parameter ``c4corpus.keepminimalhtml`` is optional. If set to ``true``, the minimal HTML tags
for each paragraph will be kept (see the example for boilerplate removal above)

* Using 32 c3.8xlarge spot instances (each 32 CPUs, thus 1024 CPUs in total), the job finished
in 22 hours (47,656 Normalized instance hours)

You can also configure the EMR cluster in the Web Console; then you only need to provide manually the
job parameters, namely path to your  ``dkpro-c4corpus-hadoop-1.0.0.jar`` with the following parameters

```
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase1FullJob \
-D mapreduce.task.timeout=36000000 -D mapreduce.map.failures.maxpercent=5 \
-D mapreduce.map.maxattempts=2 \
s3://commoncrawl/crawl-data/CC-MAIN-2015-27/segments/*/warc/*.warc.gz \
s3://your-bucket/output-path/cc-phase1out-2015-11
```

Consult http://docs.aws.amazon.com/cli/latest/reference/emr/create-cluster.html[AWS EMR Documentation] for details.


Phase 2: Exact match de-duplication::

Similarly as in the previous step, but with different parameters

```
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase2ExactMatchDeDuplication \
-D mapreduce.task.timeout=36000000 \
s3://your-bucket/output-path/cc-phase1out-2015-11/*.warc.gz \
s3://your-bucket/output-path/cc-phase2out-2015-11/
```

Took 22 minutes with 4 + 16 c3.8xlarge instances.

Phase 3: Detecting near duplicates::

Phase 3 consists of 4 steps.

.Step 1: Extract near duplicates info
----
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase3Step1ExtractNearDupInfo \
s3://your-bucket/output-path/cc-phase2out-2015-11/*.warc.gz \
s3://your-bucket/output-path/cc-phase3step1out-2015-11/
----

.Step 2: Distinct data
----
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase3Step2DistinctDataJob \
s3://your-bucket/output-path/cc-phase3step1out-2015-11/*.txt \
s3://your-bucket/output-path/cc-phase3step2out-2015-11/
----

.Step 3: Tuples creation
----
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase3Step3NearDupTuplesCreation \
-D mapreduce.task.timeout=0 \
s3://your-bucket/output-path/cc-phase3step2out-2015-11/* \
s3://your-bucket/output-path/cc-phase3step3out-2015-11/
----

* The timeout should be disabled as while calculating the Hamming distance,
the mapper neither reads an input, writes an output, nor updates its status string
so it will fail after the default 3 hours.

.Step 4: Greedy clustering
----
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase3Step4GreedyClustering \
-D mapreduce.task.timeout=0 \
s3://your-bucket/output-path/cc-phase3step3out-2015-11/* \
s3://your-bucket/output-path/cc-phase3step4out-2015-11/
----

Phase 4: Removing near duplicates::

```
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase4RemoveDuplicatesUsingReduceSideJoins \
s3://your-bucket/output-path/cc-phase3step4out-2015-11/ \
s3://your-bucket/output-path/cc-phase2out-2015-11/*.warc.gz \
s3://your-bucket/output-path/cc-phase4out-2015-11/
```

Phase 5: Sorting final corpus by language and license::

```
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.full.Phase5MergeByLangLicJob \
s3://your-bucket/output-path/cc-phase4out-2015-11/*.warc.gz \
s3://your-bucket/output-path/cc-final-2015-11/
```

==== Including C4CorpusTools in your Java projects

C4CorpusTools is hosted on Maven Central, you can add the following dependencies into your ``pom.xml``
(see descriptions above)

```
<dependency>
  <groupId>org.dkpro.c4corpus</groupId>
  <artifactId>dkpro-c4corpus-boilerplate</artifactId>
  <version>1.0.0</version>
</dependency>
```

and analogically

* ``<artifactId>dkpro-c4corpus-license</artifactId>``
* ``<artifactId>dkpro-c4corpus-deduplication</artifactId>``
* ``<artifactId>dkpro-c4corpus-language</artifactId>``
* ``<artifactId>dkpro-c4corpus-hadoop</artifactId>``

=== Working with C4Corpus - Word count example

Although you can download the C4Corpus to your computer and process it locally, it is probably
worth running it on an AWS EMR cluster (good scalability).

See ``de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.examples.WordCountExample`` under ``dkpro-c4corpus-hadoop``
which is an adaptation of the famous word counting example present in every Hadoop tutorial.

You should run it on the processed C4Corpus; here we want to count words in all German data.

* Spin an EMR cluster. It doesn't have to be big, I tested this example with 2 nodes
    ** Tested with ``emr-4.2.0`` distribution but it should work with newer ones as well
    ** Also add ``Pig 0.14.0`` if you want to analyze the output
* Run this step, change your output bucket accordingly

```
hadoop jar s3://your-bucket/dkpro-c4corpus-hadoop-1.0.0.jar \
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.examples.WordCounterExample \
s3://commoncrawl/contrib/c4corpus/CC-MAIN-2016-07/*Lang_de*.warc.gz \
s3://bucket/statistics/examples-word-count-de-2015-11/
```

This will produce several plain text files with words and its counts. The output is pretty big (320 MB)
because of many "words" with a single occurrence.

Let's explore that deeper using Pig. Login to your headnode, i.e.,

``
ssh -i your-keypair.pem hadoop@ec2-54-85-129-184.compute-1.amazonaws.com
``

and run Pig

```
[hadoop@ip-172-31-9-118 ~]$ pig
[...]
grunt> words = load 's3://ukp-research-data/c4corpus/statistics/examples-word-count-de-2015-11/'
 as (word:chararray, counts:int);
grunt> sorted = order words by counts desc;
grunt> top100 = limit sorted 100;
grunt> dump top100;
[...]
(und,43420735)
(der,38801000)
(die,36769583)
(in,24394590)
(r,16453990)
(von,15897453)
(f,15624384)
(den,15533307)
(mit,15096028)
(ist,15001207)
(zu,14588717)
(das,13847411)
(auf,11843696)
(1,11153547)
(nicht,10757865)
(im,10262142)
[...]
```

This will sort the words by their counts (revesed) and prints the top 100 words to the console.
Consult http://pig.apache.org/[Pig documentation] for further data manipulation.

=== Corpus statistics reported in the LREC article

==== Token and document counts in the final corpus

Reports in Table 7 were collected using the following M/R job:

```
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.statistics.LangLicenseStatistics
```

Run it with the following parameters on EMR cluster:

```
s3://commoncrawl/contrib/c4corpus/CC-MAIN-2016-07/*.warc.gz s3://your-bucket/statistics
```

Then download the results into a local file system and convert it to a CSV table:

```
$ aws s3 cp s3://ukp-research-data/c4corpus/statistics/cc-final-2015-11/ . --recursive
$ gunzip *.gz -c > stats.tsv
$ java -cp dkpro-c4corpus-hadoop-1.0.0.jar \
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.statistics.StatisticsTableCreator \
stats.tsv stats-table.csv
```


==== Collecting word distribution statistics

* Collect word statistics (CommonCrawl CC)
```
hadoop jar dkpro-c4corpus-hadoop-1.0.0.jar \
de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.statistics.vocabulary.WARCWordDistribution \
s3://ukp-research-data/c4corpus/statistics/cc-final-2015-11/Lic_publicdomain_Lang_en*,\
s3://ukp-research-data/c4corpus/statistics/cc-final-2015-11/Lic_cc-unspecified_Lang_en*,\
s3://ukp-research-data/c4corpus/statistics/cc-final-2015-11/Lic_by*_Lang_en* \
s3://your-bucket/output-folder1
```

* Sort vocabularies using Pig

`$ pig`

```PigLatin
a = LOAD 's3://your-bucket/output-folder1*' AS (word:chararray, counts:int);
b = order a by counts desc;
c = filter b by counts > 4;
store c into 's3://your-bucket/output-folder2' using PigStorage();
```

* Get data to your local filesystem

```
hadoop fs -getmerge s3://your-bucket/output-folder2/* cc-vocabulary-sorted.txt
```

* Compare two corpora using `de.tudarmstadt.ukp.dkpro.c4corpus.hadoop.statistics.vocabulary.TopNWordsCorrelation`
    ** parameters `brown-vocabulary-sorted.txt another-corpus.txt topNWords`


=== Collect vocabulary distribution from Wikipedia


* Download Wikipedia dump
    ** ``wget http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2``
    ** (Note: torrents are much faster)
* Run WikiExtractor.py to extract plain text (http://medialab.di.unipi.it/wiki/Wikipedia_Extractor)
    ** ``./WikiExtractor.py -c -o extracted``
* Upload Wikipedia to HDFS
```
~/wikipedia/extracted$ for prefix in * ; do for file in $prefix/* ; do echo $prefix ; echo $file; \
filename=$(basename "$file") ; echo $filename; head -1 $file; \
cat $file | hadoop fs -put - "/user/your-folder/enwiki/$prefix_$filename.txt" ; done; done
```
* We ran this step on a local Hadoop cluster; you have to adjust it to work on AWS EMR

=== Boilerplate Removal Results


==== Resources
* The Dataset is obtained from http://cleaneval.sigwac.org.uk/[the CleanEval official website]:
    ** http://cleaneval.sigwac.org.uk/finalrun-input.tar.gz[The HTML  input data]
    ** http://cleaneval.sigwac.org.uk/GoldStandard.tar.gz[The gold standard data]

* The Python original implementation of JusText, that we evaluate our jave re-implementation against, could be found on https://github.com/miso-belica/jusText[github] or https://code.google.com/archive/p/justext/[google-code] page. Moreover, JusText has an https://nlp.fi.muni.cz/projects/justext/[online demo].

* http://webascorpus.sourceforge.net/PHITE.php%3Fsitesig%3DFILES%26page%3DFILES_10_Software[CleanEval Evaluation Python script] is used to evaluate the tool and is written by Stefan Evert.

==== CleanEval Results Reproducibility
Notes::
* We evaluated our system on (only) 681 files from the original html input. The reason behind that is, only 681 files have gold standard clean text.
* ``JusText_Python_Defaults_CleanEvalHTMLTestSubset`` contains the pages obtained by running the python script of JusText on the 681 files of the cleaneval dataset
* ``JusText_Java_Defaults_CleanEvalHTMLTestSubset`` contains the pages obtained obtained by running our java implementation on the *same* 681 file of the cleaneval dataset
* To obtain the same results which are reported in the paper:
    ** To get the results of JusText (Pomikálek 2011)
```
python cleaneval.py -s -a -u  JusText_Python_Defaults_CleanEvalHTMLTestSubset CleanEvalGoldStandard
```

    ** To get the results of our java reimplementation of JusText
```
python cleaneval.py -s -a -u  JusText_Java_Defaults_CleanEvalHTMLTestSubset CleanEvalGoldStandard
```
